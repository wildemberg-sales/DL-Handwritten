{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMB3m79q6VAmBjJKsE4wk7Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wildemberg-sales/DL-Handwritten/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning para identificação de números manuscritros a partir da da base de dados MNIST"
      ],
      "metadata": {
        "id": "hxLBzKOCwYTV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETDSKEvRwVzG",
        "outputId": "424f7eff-5b27-4066-d53c-1f55d53c8435"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/611.8 kB\u001b[0m \u001b[31m722.5 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/611.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/611.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m583.7/611.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting np_utils\n",
            "  Downloading np_utils-0.6.0.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.10/dist-packages (from np_utils) (1.25.2)\n",
            "Building wheels for collected packages: np_utils\n",
            "  Building wheel for np_utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for np_utils: filename=np_utils-0.6.0-py3-none-any.whl size=56441 sha256=c3af63d4d9b6cae6b80708c80a6f8253025eb05f73db1faa33a5aad8f73a5936\n",
            "  Stored in directory: /root/.cache/pip/wheels/b6/c7/50/2307607f44366dd021209f660045f8d51cb976514d30be7cc7\n",
            "Successfully built np_utils\n",
            "Installing collected packages: np_utils\n",
            "Successfully installed np_utils-0.6.0\n"
          ]
        }
      ],
      "source": [
        "#!pip install -q -U tensorflow\n",
        "#!pip install -q -U keras\n",
        "#!pip install -q -U numpy\n",
        "#!pip install -q -U pandas\n",
        "!pip install -q -U tensorflow-addons\n",
        "!pip install -q -U keras-utils\n",
        "!pip install np_utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\n",
        "import np_utils\n",
        "from keras.datasets import mnist\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.metrics import *"
      ],
      "metadata": {
        "id": "krNlAOrfwxBw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCoAtk32wzLX",
        "outputId": "203db5ee-c798-408e-8f3d-1842f3193aba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anwDxtl0w2Ib",
        "outputId": "5b90a61a-5138-4e4c-b3d9-b097d95234d1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f-10sqVw-6b",
        "outputId": "c4f26a66-b192-43fa-f11f-9a3634a6b994"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando como é a imagem\n",
        "plt.imshow(X_train[0], cmap = 'gray')\n",
        "plt.title(str(y_train[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "p1zQrpByxBd3",
        "outputId": "8ca71b57-a6eb-4d31-f9c5-aae08569adf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, '5')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# redimensiona o formato para ser compatível com o que o modelo precisa\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n"
      ],
      "metadata": {
        "id": "sB3oZFhKxMR3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificando o formato que ficaram os dados\n",
        "print(X_train.shape) # (60000, 28, 28, 1) são 60000 imagens, de 28x28 pixels, e um canal de cor (preto e branco)\n",
        "print(X_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HORItEUOxP2J",
        "outputId": "d7f2fb47-3ede-41e5-ca28-6631c94133dc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converte cada pixel da imagem para um tipo mais fácil de ser processado\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n"
      ],
      "metadata": {
        "id": "-7LaMXSYxhbA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalização dos dados\n",
        "X_train /= 255\n",
        "X_test /= 255\n"
      ],
      "metadata": {
        "id": "47TKZRlBxjto"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2tlE6QRxnoE",
        "outputId": "616bc318-3e6a-4ab8-ec60-0f3149d55026"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.01176471]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.07058824]\n",
            "  [0.49411765]\n",
            "  [0.53333336]\n",
            "  [0.6862745 ]\n",
            "  [0.10196079]\n",
            "  [0.6509804 ]\n",
            "  [1.        ]\n",
            "  [0.96862745]\n",
            "  [0.49803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.11764706]\n",
            "  [0.14117648]\n",
            "  [0.36862746]\n",
            "  [0.6039216 ]\n",
            "  [0.6666667 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.88235295]\n",
            "  [0.6745098 ]\n",
            "  [0.99215686]\n",
            "  [0.9490196 ]\n",
            "  [0.7647059 ]\n",
            "  [0.2509804 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.19215687]\n",
            "  [0.93333334]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.9843137 ]\n",
            "  [0.3647059 ]\n",
            "  [0.32156864]\n",
            "  [0.32156864]\n",
            "  [0.21960784]\n",
            "  [0.15294118]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.85882354]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7764706 ]\n",
            "  [0.7137255 ]\n",
            "  [0.96862745]\n",
            "  [0.94509804]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.3137255 ]\n",
            "  [0.6117647 ]\n",
            "  [0.41960785]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.8039216 ]\n",
            "  [0.04313726]\n",
            "  [0.        ]\n",
            "  [0.16862746]\n",
            "  [0.6039216 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.05490196]\n",
            "  [0.00392157]\n",
            "  [0.6039216 ]\n",
            "  [0.99215686]\n",
            "  [0.3529412 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.54509807]\n",
            "  [0.99215686]\n",
            "  [0.74509805]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.04313726]\n",
            "  [0.74509805]\n",
            "  [0.99215686]\n",
            "  [0.27450982]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.13725491]\n",
            "  [0.94509804]\n",
            "  [0.88235295]\n",
            "  [0.627451  ]\n",
            "  [0.42352942]\n",
            "  [0.00392157]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.31764707]\n",
            "  [0.9411765 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.46666667]\n",
            "  [0.09803922]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.1764706 ]\n",
            "  [0.7294118 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.5882353 ]\n",
            "  [0.10588235]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.0627451 ]\n",
            "  [0.3647059 ]\n",
            "  [0.9882353 ]\n",
            "  [0.99215686]\n",
            "  [0.73333335]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.9764706 ]\n",
            "  [0.99215686]\n",
            "  [0.9764706 ]\n",
            "  [0.2509804 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.18039216]\n",
            "  [0.50980395]\n",
            "  [0.7176471 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.8117647 ]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.15294118]\n",
            "  [0.5803922 ]\n",
            "  [0.8980392 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.98039216]\n",
            "  [0.7137255 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09411765]\n",
            "  [0.44705883]\n",
            "  [0.8666667 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7882353 ]\n",
            "  [0.30588236]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.09019608]\n",
            "  [0.25882354]\n",
            "  [0.8352941 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7764706 ]\n",
            "  [0.31764707]\n",
            "  [0.00784314]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.07058824]\n",
            "  [0.67058825]\n",
            "  [0.85882354]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.7647059 ]\n",
            "  [0.3137255 ]\n",
            "  [0.03529412]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.21568628]\n",
            "  [0.6745098 ]\n",
            "  [0.8862745 ]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.95686275]\n",
            "  [0.52156866]\n",
            "  [0.04313726]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.53333336]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.99215686]\n",
            "  [0.83137256]\n",
            "  [0.5294118 ]\n",
            "  [0.5176471 ]\n",
            "  [0.0627451 ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]\n",
            "\n",
            " [[0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]\n",
            "  [0.        ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando um modelo de categorização dos meus dados a partir do seu resultado\n",
        "# No total são 10 classes que vão de 0 a 5\n",
        "n_classes = tf.keras.utils.to_categorical(y_train, 10)\n",
        "print(n_classes)\n",
        "print(y_train[0])\n",
        "print(n_classes[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9j2PqXPxr3g",
        "outputId": "360b8b00-de1a-4a25-958a-ee26d89dbb53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [1. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 1. 0.]]\n",
            "5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Repetindo para os dados de teste\n",
        "n_classes_test = tf.keras.utils.to_categorical(y_test, 10)\n"
      ],
      "metadata": {
        "id": "z1-nlTAMx8uG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição da arquitetura do modelo\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters = 64,\n",
        "                 kernel_size = (3, 3),\n",
        "                 input_shape = (28, 28, 1), # 28x28 pixels com um canal de cor\n",
        "                 activation = 'relu'))\n",
        "model.add(BatchNormalization()) # Polimento dos dados usando normalização\n",
        "model.add(MaxPooling2D(pool_size = (2, 2))) # reduz a dimensão dos mapas de características\n",
        "model.add(Conv2D(filters = 256,\n",
        "                 kernel_size = (3, 3),\n",
        "                 activation = 'relu'))\n",
        "model.add(Conv2D(filters = 256,\n",
        "                 kernel_size = (3, 3),\n",
        "                 activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "# fim da alimentação do modelo e extração de características\n",
        "# Ínicio do treinamento do modelo\n",
        "model.add(Flatten()) #achatamento para a próxima camada para encontrar os melhores pesos\n",
        "model.add(Dense(units = 256,\n",
        "                activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 256,\n",
        "                activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(units = 128,\n",
        "                activation = 'relu'))\n",
        "model.add(Dense(units = 10,\n",
        "                activation = 'softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qyqtJ8rnyMtf",
        "outputId": "8cb724a7-c669-4a70-bfe4-a89d11dc8edd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 26, 26, 64)        640       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 26, 26, 64)        256       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 13, 13, 64)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 11, 11, 256)       147712    \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 9, 9, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 9, 9, 256)         1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 4, 4, 256)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               1048832   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               65792     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1888522 (7.20 MB)\n",
            "Trainable params: 1887882 (7.20 MB)\n",
            "Non-trainable params: 640 (2.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definição das métricas que serão utilizadas para acompanhar o modelo\n",
        "METRICS = [CategoricalAccuracy(name = 'accuracy'),\n",
        "           TruePositives(thresholds = 0.5, name = 'tp'),\n",
        "           TrueNegatives(thresholds = 0.5, name = 'tn'),\n",
        "           FalsePositives(thresholds = 0.5, name = 'fp'),\n",
        "           FalseNegatives(thresholds = 0.5, name = 'fn'),\n",
        "           PrecisionAtRecall(recall = 0.5, name = 'precision'),\n",
        "           SensitivityAtSpecificity(0.5, name = 'sensitivity'),\n",
        "           SpecificityAtSensitivity(sensitivity = 0.5,\n",
        "                                    name = 'specificity'),\n",
        "           Recall(name='recall')]\n"
      ],
      "metadata": {
        "id": "nvqGm2mx0sNF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilação do Modelo\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = 'adam',\n",
        "              metrics = METRICS)"
      ],
      "metadata": {
        "id": "LuyqMWuy01Py"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aumento sintético da base de dados\n",
        "# Cria novas imagens baseadas na existente aplicando alterações\n",
        "gerador_treinamento = ImageDataGenerator(rotation_range = 7,\n",
        "                                         horizontal_flip = True,\n",
        "                                         shear_range = 0.2,\n",
        "                                         height_shift_range = 0.07,\n",
        "                                         zoom_range = 0.2)\n",
        "\n",
        "# aqui ela decide as alterações arbitrariamente\n",
        "gerador_teste = ImageDataGenerator()\n",
        "\n",
        "X_train = gerador_treinamento.flow(X_train,\n",
        "                                   n_classes,\n",
        "                                   batch_size = 128)\n",
        "\n",
        "X_test = gerador_teste.flow(X_test,\n",
        "                            n_classes_test,\n",
        "                            batch_size = 128)\n"
      ],
      "metadata": {
        "id": "9Aueleg21gd6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hist = model.fit_generator(X_train,\n",
        "                           steps_per_epoch = 60000 / 128,\n",
        "                           epochs = 100,\n",
        "                           validation_data = X_test,\n",
        "                           validation_steps = 10000 / 128)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "a3qzHKwL1kKc",
        "outputId": "88f0a5ad-98e4-4921-ef72-d06baaba6fc5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-ace0db55ed34>:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  hist = model.fit_generator(X_train,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "468/468 [==============================] - 45s 59ms/step - loss: 0.2987 - accuracy: 0.9068 - tp: 53180.0000 - tn: 536290.0000 - fp: 3710.0000 - fn: 6820.0000 - precision: 0.9963 - sensitivity: 0.9941 - specificity: 0.9998 - recall: 0.8863 - val_loss: 0.3434 - val_accuracy: 0.9036 - val_tp: 8943.0000 - val_tn: 89145.0000 - val_fp: 855.0000 - val_fn: 1057.0000 - val_precision: 0.9790 - val_sensitivity: 0.9858 - val_specificity: 0.9987 - val_recall: 0.8943\n",
            "Epoch 2/100\n",
            "468/468 [==============================] - 23s 49ms/step - loss: 0.1238 - accuracy: 0.9630 - tp: 57457.0000 - tn: 538216.0000 - fp: 1784.0000 - fn: 2543.0000 - precision: 0.9980 - sensitivity: 0.9966 - specificity: 0.9998 - recall: 0.9576 - val_loss: 0.0685 - val_accuracy: 0.9778 - val_tp: 9750.0000 - val_tn: 89816.0000 - val_fp: 184.0000 - val_fn: 250.0000 - val_precision: 0.9995 - val_sensitivity: 0.9989 - val_specificity: 1.0000 - val_recall: 0.9750\n",
            "Epoch 3/100\n",
            "468/468 [==============================] - 23s 49ms/step - loss: 0.0970 - accuracy: 0.9706 - tp: 57993.0000 - tn: 538518.0000 - fp: 1482.0000 - fn: 2007.0000 - precision: 0.9982 - sensitivity: 0.9973 - specificity: 0.9998 - recall: 0.9665 - val_loss: 0.0482 - val_accuracy: 0.9857 - val_tp: 9832.0000 - val_tn: 89878.0000 - val_fp: 122.0000 - val_fn: 168.0000 - val_precision: 0.9993 - val_sensitivity: 0.9992 - val_specificity: 0.9999 - val_recall: 0.9832\n",
            "Epoch 4/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0863 - accuracy: 0.9739 - tp: 58246.0000 - tn: 538699.0000 - fp: 1301.0000 - fn: 1754.0000 - precision: 0.9984 - sensitivity: 0.9975 - specificity: 0.9999 - recall: 0.9708 - val_loss: 0.0473 - val_accuracy: 0.9867 - val_tp: 9840.0000 - val_tn: 89886.0000 - val_fp: 114.0000 - val_fn: 160.0000 - val_precision: 0.9988 - val_sensitivity: 0.9987 - val_specificity: 0.9999 - val_recall: 0.9840\n",
            "Epoch 5/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0770 - accuracy: 0.9776 - tp: 58453.0000 - tn: 538862.0000 - fp: 1138.0000 - fn: 1547.0000 - precision: 0.9985 - sensitivity: 0.9978 - specificity: 0.9999 - recall: 0.9742 - val_loss: 0.0344 - val_accuracy: 0.9885 - val_tp: 9875.0000 - val_tn: 89903.0000 - val_fp: 97.0000 - val_fn: 125.0000 - val_precision: 0.9993 - val_sensitivity: 0.9992 - val_specificity: 0.9999 - val_recall: 0.9875\n",
            "Epoch 6/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0694 - accuracy: 0.9797 - tp: 58602.0000 - tn: 538975.0000 - fp: 1025.0000 - fn: 1398.0000 - precision: 0.9985 - sensitivity: 0.9980 - specificity: 0.9999 - recall: 0.9767 - val_loss: 0.0388 - val_accuracy: 0.9882 - val_tp: 9869.0000 - val_tn: 89896.0000 - val_fp: 104.0000 - val_fn: 131.0000 - val_precision: 0.9990 - val_sensitivity: 0.9989 - val_specificity: 0.9999 - val_recall: 0.9869\n",
            "Epoch 7/100\n",
            "468/468 [==============================] - 23s 48ms/step - loss: 0.0663 - accuracy: 0.9806 - tp: 58672.0000 - tn: 539019.0000 - fp: 981.0000 - fn: 1328.0000 - precision: 0.9985 - sensitivity: 0.9981 - specificity: 0.9999 - recall: 0.9779 - val_loss: 0.0647 - val_accuracy: 0.9802 - val_tp: 9772.0000 - val_tn: 89837.0000 - val_fp: 163.0000 - val_fn: 228.0000 - val_precision: 0.9989 - val_sensitivity: 0.9985 - val_specificity: 0.9999 - val_recall: 0.9772\n",
            "Epoch 8/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0633 - accuracy: 0.9810 - tp: 58695.0000 - tn: 539021.0000 - fp: 979.0000 - fn: 1305.0000 - precision: 0.9987 - sensitivity: 0.9981 - specificity: 0.9999 - recall: 0.9783 - val_loss: 0.0418 - val_accuracy: 0.9870 - val_tp: 9858.0000 - val_tn: 89887.0000 - val_fp: 113.0000 - val_fn: 142.0000 - val_precision: 0.9994 - val_sensitivity: 0.9992 - val_specificity: 0.9999 - val_recall: 0.9858\n",
            "Epoch 9/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0623 - accuracy: 0.9824 - tp: 58803.0000 - tn: 539094.0000 - fp: 906.0000 - fn: 1197.0000 - precision: 0.9987 - sensitivity: 0.9980 - specificity: 0.9999 - recall: 0.9801 - val_loss: 0.0631 - val_accuracy: 0.9824 - val_tp: 9774.0000 - val_tn: 89861.0000 - val_fp: 139.0000 - val_fn: 226.0000 - val_precision: 0.9988 - val_sensitivity: 0.9983 - val_specificity: 0.9999 - val_recall: 0.9774\n",
            "Epoch 10/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0577 - accuracy: 0.9829 - tp: 58849.0000 - tn: 539122.0000 - fp: 878.0000 - fn: 1151.0000 - precision: 0.9990 - sensitivity: 0.9983 - specificity: 0.9999 - recall: 0.9808 - val_loss: 0.0482 - val_accuracy: 0.9863 - val_tp: 9847.0000 - val_tn: 89886.0000 - val_fp: 114.0000 - val_fn: 153.0000 - val_precision: 0.9992 - val_sensitivity: 0.9987 - val_specificity: 0.9999 - val_recall: 0.9847\n",
            "Epoch 11/100\n",
            "468/468 [==============================] - 31s 66ms/step - loss: 0.0551 - accuracy: 0.9835 - tp: 58901.0000 - tn: 539168.0000 - fp: 832.0000 - fn: 1099.0000 - precision: 0.9989 - sensitivity: 0.9984 - specificity: 0.9999 - recall: 0.9817 - val_loss: 0.0399 - val_accuracy: 0.9869 - val_tp: 9852.0000 - val_tn: 89885.0000 - val_fp: 115.0000 - val_fn: 148.0000 - val_precision: 0.9996 - val_sensitivity: 0.9994 - val_specificity: 1.0000 - val_recall: 0.9852\n",
            "Epoch 12/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0531 - accuracy: 0.9847 - tp: 58956.0000 - tn: 539213.0000 - fp: 787.0000 - fn: 1044.0000 - precision: 0.9990 - sensitivity: 0.9985 - specificity: 0.9999 - recall: 0.9826 - val_loss: 0.0431 - val_accuracy: 0.9866 - val_tp: 9845.0000 - val_tn: 89892.0000 - val_fp: 108.0000 - val_fn: 155.0000 - val_precision: 0.9994 - val_sensitivity: 0.9990 - val_specificity: 0.9999 - val_recall: 0.9845\n",
            "Epoch 13/100\n",
            "468/468 [==============================] - 23s 48ms/step - loss: 0.0505 - accuracy: 0.9849 - tp: 58986.0000 - tn: 539214.0000 - fp: 786.0000 - fn: 1014.0000 - precision: 0.9989 - sensitivity: 0.9986 - specificity: 0.9999 - recall: 0.9831 - val_loss: 0.0398 - val_accuracy: 0.9871 - val_tp: 9852.0000 - val_tn: 89883.0000 - val_fp: 117.0000 - val_fn: 148.0000 - val_precision: 0.9992 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9852\n",
            "Epoch 14/100\n",
            "468/468 [==============================] - 22s 46ms/step - loss: 0.0497 - accuracy: 0.9848 - tp: 59002.0000 - tn: 539201.0000 - fp: 799.0000 - fn: 998.0000 - precision: 0.9989 - sensitivity: 0.9988 - specificity: 0.9999 - recall: 0.9834 - val_loss: 0.0534 - val_accuracy: 0.9828 - val_tp: 9806.0000 - val_tn: 89861.0000 - val_fp: 139.0000 - val_fn: 194.0000 - val_precision: 0.9995 - val_sensitivity: 0.9994 - val_specificity: 1.0000 - val_recall: 0.9806\n",
            "Epoch 15/100\n",
            "468/468 [==============================] - 23s 49ms/step - loss: 0.0476 - accuracy: 0.9859 - tp: 59057.0000 - tn: 539266.0000 - fp: 734.0000 - fn: 943.0000 - precision: 0.9992 - sensitivity: 0.9988 - specificity: 0.9999 - recall: 0.9843 - val_loss: 0.0324 - val_accuracy: 0.9891 - val_tp: 9878.0000 - val_tn: 89908.0000 - val_fp: 92.0000 - val_fn: 122.0000 - val_precision: 0.9996 - val_sensitivity: 0.9995 - val_specificity: 1.0000 - val_recall: 0.9878\n",
            "Epoch 16/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0453 - accuracy: 0.9868 - tp: 59105.0000 - tn: 539313.0000 - fp: 687.0000 - fn: 895.0000 - precision: 0.9990 - sensitivity: 0.9986 - specificity: 0.9999 - recall: 0.9851 - val_loss: 0.0717 - val_accuracy: 0.9803 - val_tp: 9768.0000 - val_tn: 89847.0000 - val_fp: 153.0000 - val_fn: 232.0000 - val_precision: 0.9986 - val_sensitivity: 0.9984 - val_specificity: 0.9999 - val_recall: 0.9768\n",
            "Epoch 17/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0441 - accuracy: 0.9870 - tp: 59126.0000 - tn: 539332.0000 - fp: 668.0000 - fn: 874.0000 - precision: 0.9992 - sensitivity: 0.9987 - specificity: 0.9999 - recall: 0.9854 - val_loss: 0.0302 - val_accuracy: 0.9896 - val_tp: 9889.0000 - val_tn: 89915.0000 - val_fp: 85.0000 - val_fn: 111.0000 - val_precision: 0.9993 - val_sensitivity: 0.9994 - val_specificity: 0.9999 - val_recall: 0.9889\n",
            "Epoch 18/100\n",
            "468/468 [==============================] - 21s 46ms/step - loss: 0.0434 - accuracy: 0.9872 - tp: 59143.0000 - tn: 539325.0000 - fp: 675.0000 - fn: 857.0000 - precision: 0.9993 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9857 - val_loss: 0.0446 - val_accuracy: 0.9860 - val_tp: 9844.0000 - val_tn: 89883.0000 - val_fp: 117.0000 - val_fn: 156.0000 - val_precision: 0.9991 - val_sensitivity: 0.9981 - val_specificity: 0.9999 - val_recall: 0.9844\n",
            "Epoch 19/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0414 - accuracy: 0.9875 - tp: 59175.0000 - tn: 539349.0000 - fp: 651.0000 - fn: 825.0000 - precision: 0.9992 - sensitivity: 0.9989 - specificity: 0.9999 - recall: 0.9862 - val_loss: 0.0311 - val_accuracy: 0.9904 - val_tp: 9896.0000 - val_tn: 89913.0000 - val_fp: 87.0000 - val_fn: 104.0000 - val_precision: 0.9995 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9896\n",
            "Epoch 20/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0448 - accuracy: 0.9864 - tp: 59095.0000 - tn: 539285.0000 - fp: 715.0000 - fn: 905.0000 - precision: 0.9991 - sensitivity: 0.9988 - specificity: 0.9999 - recall: 0.9849 - val_loss: 0.0335 - val_accuracy: 0.9889 - val_tp: 9880.0000 - val_tn: 89902.0000 - val_fp: 98.0000 - val_fn: 120.0000 - val_precision: 0.9992 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9880\n",
            "Epoch 21/100\n",
            "468/468 [==============================] - 24s 50ms/step - loss: 0.0398 - accuracy: 0.9879 - tp: 59186.0000 - tn: 539365.0000 - fp: 635.0000 - fn: 814.0000 - precision: 0.9992 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9864 - val_loss: 0.0342 - val_accuracy: 0.9901 - val_tp: 9889.0000 - val_tn: 89911.0000 - val_fp: 89.0000 - val_fn: 111.0000 - val_precision: 0.9993 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9889\n",
            "Epoch 22/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0395 - accuracy: 0.9885 - tp: 59224.0000 - tn: 539407.0000 - fp: 593.0000 - fn: 776.0000 - precision: 0.9991 - sensitivity: 0.9988 - specificity: 0.9999 - recall: 0.9871 - val_loss: 0.0274 - val_accuracy: 0.9915 - val_tp: 9909.0000 - val_tn: 89928.0000 - val_fp: 72.0000 - val_fn: 91.0000 - val_precision: 0.9994 - val_sensitivity: 0.9990 - val_specificity: 0.9999 - val_recall: 0.9909\n",
            "Epoch 23/100\n",
            "468/468 [==============================] - 23s 49ms/step - loss: 0.0371 - accuracy: 0.9892 - tp: 59273.0000 - tn: 539428.0000 - fp: 572.0000 - fn: 727.0000 - precision: 0.9993 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9879 - val_loss: 0.0406 - val_accuracy: 0.9869 - val_tp: 9848.0000 - val_tn: 89902.0000 - val_fp: 98.0000 - val_fn: 152.0000 - val_precision: 0.9998 - val_sensitivity: 0.9997 - val_specificity: 1.0000 - val_recall: 0.9848\n",
            "Epoch 24/100\n",
            "468/468 [==============================] - 24s 51ms/step - loss: 0.0381 - accuracy: 0.9885 - tp: 59235.0000 - tn: 539373.0000 - fp: 627.0000 - fn: 765.0000 - precision: 0.9993 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9872 - val_loss: 0.0417 - val_accuracy: 0.9880 - val_tp: 9865.0000 - val_tn: 89909.0000 - val_fp: 91.0000 - val_fn: 135.0000 - val_precision: 0.9994 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9865\n",
            "Epoch 25/100\n",
            "468/468 [==============================] - 22s 48ms/step - loss: 0.0375 - accuracy: 0.9891 - tp: 59279.0000 - tn: 539429.0000 - fp: 571.0000 - fn: 721.0000 - precision: 0.9992 - sensitivity: 0.9988 - specificity: 0.9999 - recall: 0.9880 - val_loss: 0.0374 - val_accuracy: 0.9896 - val_tp: 9881.0000 - val_tn: 89911.0000 - val_fp: 89.0000 - val_fn: 119.0000 - val_precision: 0.9987 - val_sensitivity: 0.9987 - val_specificity: 0.9999 - val_recall: 0.9881\n",
            "Epoch 26/100\n",
            "468/468 [==============================] - 23s 50ms/step - loss: 0.0356 - accuracy: 0.9893 - tp: 59294.0000 - tn: 539436.0000 - fp: 564.0000 - fn: 706.0000 - precision: 0.9991 - sensitivity: 0.9989 - specificity: 0.9999 - recall: 0.9882 - val_loss: 0.0320 - val_accuracy: 0.9898 - val_tp: 9888.0000 - val_tn: 89911.0000 - val_fp: 89.0000 - val_fn: 112.0000 - val_precision: 0.9995 - val_sensitivity: 0.9994 - val_specificity: 0.9999 - val_recall: 0.9888\n",
            "Epoch 27/100\n",
            "468/468 [==============================] - 23s 48ms/step - loss: 0.0363 - accuracy: 0.9887 - tp: 59259.0000 - tn: 539409.0000 - fp: 591.0000 - fn: 741.0000 - precision: 0.9993 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9876 - val_loss: 0.0488 - val_accuracy: 0.9849 - val_tp: 9832.0000 - val_tn: 89871.0000 - val_fp: 129.0000 - val_fn: 168.0000 - val_precision: 0.9993 - val_sensitivity: 0.9985 - val_specificity: 0.9999 - val_recall: 0.9832\n",
            "Epoch 28/100\n",
            "468/468 [==============================] - 22s 47ms/step - loss: 0.0344 - accuracy: 0.9894 - tp: 59301.0000 - tn: 539427.0000 - fp: 573.0000 - fn: 699.0000 - precision: 0.9994 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9883 - val_loss: 0.0339 - val_accuracy: 0.9893 - val_tp: 9883.0000 - val_tn: 89913.0000 - val_fp: 87.0000 - val_fn: 117.0000 - val_precision: 0.9993 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9883\n",
            "Epoch 29/100\n",
            "468/468 [==============================] - 23s 48ms/step - loss: 0.0360 - accuracy: 0.9891 - tp: 59276.0000 - tn: 539418.0000 - fp: 582.0000 - fn: 724.0000 - precision: 0.9992 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9879 - val_loss: 0.0291 - val_accuracy: 0.9916 - val_tp: 9911.0000 - val_tn: 89922.0000 - val_fp: 78.0000 - val_fn: 89.0000 - val_precision: 0.9997 - val_sensitivity: 0.9993 - val_specificity: 1.0000 - val_recall: 0.9911\n",
            "Epoch 30/100\n",
            "468/468 [==============================] - 25s 53ms/step - loss: 0.0320 - accuracy: 0.9903 - tp: 59362.0000 - tn: 539484.0000 - fp: 516.0000 - fn: 638.0000 - precision: 0.9993 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9894 - val_loss: 0.0304 - val_accuracy: 0.9912 - val_tp: 9906.0000 - val_tn: 89923.0000 - val_fp: 77.0000 - val_fn: 94.0000 - val_precision: 0.9995 - val_sensitivity: 0.9992 - val_specificity: 0.9999 - val_recall: 0.9906\n",
            "Epoch 31/100\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.0348 - accuracy: 0.9898 - tp: 59305.0000 - tn: 539454.0000 - fp: 546.0000 - fn: 695.0000 - precision: 0.9994 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9884 - val_loss: 0.0327 - val_accuracy: 0.9908 - val_tp: 9900.0000 - val_tn: 89915.0000 - val_fp: 85.0000 - val_fn: 100.0000 - val_precision: 0.9993 - val_sensitivity: 0.9991 - val_specificity: 0.9999 - val_recall: 0.9900\n",
            "Epoch 32/100\n",
            "468/468 [==============================] - 31s 66ms/step - loss: 0.0329 - accuracy: 0.9902 - tp: 59348.0000 - tn: 539492.0000 - fp: 508.0000 - fn: 652.0000 - precision: 0.9992 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9891 - val_loss: 0.0475 - val_accuracy: 0.9874 - val_tp: 9859.0000 - val_tn: 89895.0000 - val_fp: 105.0000 - val_fn: 141.0000 - val_precision: 0.9988 - val_sensitivity: 0.9981 - val_specificity: 0.9999 - val_recall: 0.9859\n",
            "Epoch 33/100\n",
            "468/468 [==============================] - 31s 65ms/step - loss: 0.0313 - accuracy: 0.9906 - tp: 59398.0000 - tn: 539504.0000 - fp: 496.0000 - fn: 602.0000 - precision: 0.9992 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9900 - val_loss: 0.0325 - val_accuracy: 0.9900 - val_tp: 9893.0000 - val_tn: 89912.0000 - val_fp: 88.0000 - val_fn: 107.0000 - val_precision: 0.9988 - val_sensitivity: 0.9987 - val_specificity: 0.9999 - val_recall: 0.9893\n",
            "Epoch 34/100\n",
            "468/468 [==============================] - 31s 65ms/step - loss: 0.0298 - accuracy: 0.9908 - tp: 59411.0000 - tn: 539506.0000 - fp: 494.0000 - fn: 589.0000 - precision: 0.9993 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9902 - val_loss: 0.0324 - val_accuracy: 0.9905 - val_tp: 9900.0000 - val_tn: 89914.0000 - val_fp: 86.0000 - val_fn: 100.0000 - val_precision: 0.9988 - val_sensitivity: 0.9987 - val_specificity: 0.9999 - val_recall: 0.9900\n",
            "Epoch 35/100\n",
            "468/468 [==============================] - 31s 67ms/step - loss: 0.0323 - accuracy: 0.9907 - tp: 59379.0000 - tn: 539500.0000 - fp: 500.0000 - fn: 621.0000 - precision: 0.9993 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9897 - val_loss: 0.0347 - val_accuracy: 0.9903 - val_tp: 9892.0000 - val_tn: 89909.0000 - val_fp: 91.0000 - val_fn: 108.0000 - val_precision: 0.9990 - val_sensitivity: 0.9989 - val_specificity: 0.9999 - val_recall: 0.9892\n",
            "Epoch 36/100\n",
            "468/468 [==============================] - 29s 61ms/step - loss: 0.0294 - accuracy: 0.9914 - tp: 59436.0000 - tn: 539532.0000 - fp: 468.0000 - fn: 564.0000 - precision: 0.9993 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9906 - val_loss: 0.0482 - val_accuracy: 0.9868 - val_tp: 9857.0000 - val_tn: 89886.0000 - val_fp: 114.0000 - val_fn: 143.0000 - val_precision: 0.9990 - val_sensitivity: 0.9981 - val_specificity: 0.9999 - val_recall: 0.9857\n",
            "Epoch 37/100\n",
            "468/468 [==============================] - 29s 63ms/step - loss: 0.0304 - accuracy: 0.9912 - tp: 59419.0000 - tn: 539529.0000 - fp: 471.0000 - fn: 581.0000 - precision: 0.9993 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9903 - val_loss: 0.0309 - val_accuracy: 0.9910 - val_tp: 9900.0000 - val_tn: 89915.0000 - val_fp: 85.0000 - val_fn: 100.0000 - val_precision: 0.9992 - val_sensitivity: 0.9988 - val_specificity: 0.9999 - val_recall: 0.9900\n",
            "Epoch 38/100\n",
            "468/468 [==============================] - 30s 64ms/step - loss: 0.0301 - accuracy: 0.9909 - tp: 59395.0000 - tn: 539504.0000 - fp: 496.0000 - fn: 605.0000 - precision: 0.9993 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9899 - val_loss: 0.0309 - val_accuracy: 0.9920 - val_tp: 9910.0000 - val_tn: 89926.0000 - val_fp: 74.0000 - val_fn: 90.0000 - val_precision: 0.9992 - val_sensitivity: 0.9989 - val_specificity: 0.9999 - val_recall: 0.9910\n",
            "Epoch 39/100\n",
            "468/468 [==============================] - 29s 62ms/step - loss: 0.0304 - accuracy: 0.9906 - tp: 59389.0000 - tn: 539499.0000 - fp: 501.0000 - fn: 611.0000 - precision: 0.9993 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9898 - val_loss: 0.0335 - val_accuracy: 0.9914 - val_tp: 9906.0000 - val_tn: 89918.0000 - val_fp: 82.0000 - val_fn: 94.0000 - val_precision: 0.9989 - val_sensitivity: 0.9988 - val_specificity: 0.9999 - val_recall: 0.9906\n",
            "Epoch 40/100\n",
            "468/468 [==============================] - 27s 58ms/step - loss: 0.0290 - accuracy: 0.9918 - tp: 59440.0000 - tn: 539561.0000 - fp: 439.0000 - fn: 560.0000 - precision: 0.9994 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9907 - val_loss: 0.0336 - val_accuracy: 0.9889 - val_tp: 9874.0000 - val_tn: 89904.0000 - val_fp: 96.0000 - val_fn: 126.0000 - val_precision: 0.9995 - val_sensitivity: 0.9993 - val_specificity: 0.9999 - val_recall: 0.9874\n",
            "Epoch 41/100\n",
            "468/468 [==============================] - 25s 54ms/step - loss: 0.0295 - accuracy: 0.9914 - tp: 59445.0000 - tn: 539524.0000 - fp: 476.0000 - fn: 555.0000 - precision: 0.9992 - sensitivity: 0.9990 - specificity: 0.9999 - recall: 0.9908 - val_loss: 0.0332 - val_accuracy: 0.9909 - val_tp: 9903.0000 - val_tn: 89913.0000 - val_fp: 87.0000 - val_fn: 97.0000 - val_precision: 0.9990 - val_sensitivity: 0.9987 - val_specificity: 0.9999 - val_recall: 0.9903\n",
            "Epoch 42/100\n",
            "468/468 [==============================] - 24s 50ms/step - loss: 0.0291 - accuracy: 0.9913 - tp: 59421.0000 - tn: 539536.0000 - fp: 464.0000 - fn: 579.0000 - precision: 0.9995 - sensitivity: 0.9992 - specificity: 0.9999 - recall: 0.9904 - val_loss: 0.0343 - val_accuracy: 0.9912 - val_tp: 9906.0000 - val_tn: 89920.0000 - val_fp: 80.0000 - val_fn: 94.0000 - val_precision: 0.9992 - val_sensitivity: 0.9986 - val_specificity: 0.9999 - val_recall: 0.9906\n",
            "Epoch 43/100\n",
            "468/468 [==============================] - 24s 52ms/step - loss: 0.0292 - accuracy: 0.9917 - tp: 59452.0000 - tn: 539573.0000 - fp: 427.0000 - fn: 548.0000 - precision: 0.9994 - sensitivity: 0.9991 - specificity: 0.9999 - recall: 0.9909 - val_loss: 0.0268 - val_accuracy: 0.9923 - val_tp: 9918.0000 - val_tn: 89935.0000 - val_fp: 65.0000 - val_fn: 82.0000 - val_precision: 0.9997 - val_sensitivity: 0.9992 - val_specificity: 1.0000 - val_recall: 0.9918\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ace0db55ed34>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = model.fit_generator(X_train,\n\u001b[0m\u001b[1;32m      2\u001b[0m                            \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m60000\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                            \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            validation_steps = 10000 / 128)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2911\u001b[0m             \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m         )\n\u001b[0;32m-> 2913\u001b[0;31m         return self.fit(\n\u001b[0m\u001b[1;32m   2914\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2915\u001b[0m             \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1793\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1794\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1795\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1796\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2701\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m         \"\"\"Resets the state of all the metrics in the model.\n\u001b[1;32m   2705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = hist.history['accuracy']\n",
        "loss =  hist.history['loss']\n",
        "fp = hist.history['fp']\n",
        "fpv = hist.history['val_fp']\n",
        "fn = hist.history['fn']\n",
        "fnv = hist.history['val_fn']\n",
        "tp = hist.history['tp']\n",
        "tpv = hist.history['val_tp']\n",
        "tn = hist.history['tn']\n",
        "tnv = hist.history['val_tn']\n",
        "FP = hist.history['fp'][-1]\n",
        "FN = hist.history['fn'][-1]\n",
        "TP = hist.history['tp'][-1]\n",
        "TN = hist.history['tn'][-1]\n",
        "LOSS = hist.history['loss'][-1]\n",
        "LOSSV = hist.history['val_loss'][-1]\n",
        "ACC = hist.history['accuracy'][-1]\n",
        "ACCV = hist.history['val_accuracy'][-1]\n",
        "PRE = hist.history['precision'][-1]\n",
        "PREV = hist.history['val_precision'][-1]\n",
        "REC = hist.history['recall'][-1]\n",
        "RECV = hist.history['val_recall'][-1]\n"
      ],
      "metadata": {
        "id": "rWXlqzZb1rzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accU10 = mean(acc[-10])\n",
        "tpU10 = mean(tp[-10])\n",
        "fpU10 = mean(fp[-10])\n",
        "tnU10 = mean(tn[-10])\n",
        "fnU10 = mean(fn[-10])\n",
        "\n",
        "print(f'Verdadeiros Positivos: {tpU10}')\n",
        "print(f'Falsos Positivos: {fpU10}')\n",
        "print(f'Verdadeiros Negativos: {tnU10}')\n",
        "print(f'Falsos Negativos: {fnU10}')\n",
        "\n",
        "print('--------------------')\n",
        "\n",
        "print(\"Matriz de Confusão\")\n",
        "print('*Média últimas 10 épocas de processamento')\n",
        "print(f\"[{tpU10}] [{fpU10}]\")\n",
        "print(f\"[{fnU10}] [{tnU10}]\")\n",
        "\n",
        "print('--------------------')\n",
        "\n",
        "print(f'Acurácia da Matriz de Confusão: {round(accU10, 2)*100-2}%')\n"
      ],
      "metadata": {
        "id": "NA4veINk1zU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TPR = TP /(TP + FN)\n",
        "TNR = TN /(TN + FP)\n",
        "PPV = TP /(TP + FP)\n",
        "NPV = TN /(TN + FN)\n",
        "FPR = FP /(FP + TN)\n",
        "FNR = FN /(TP + FN)\n",
        "FDR = FP /(TP + FP)\n",
        "\n",
        "OACC = (TP + TN) /(TP + FP + FN + TN)\n",
        "ACCCM = (TP + TN) / (TP + TN + FP + FN)\n",
        "FM = (2 * PRE * REC) / (PRE + REC)\n",
        "F1S = 2*((PRE * REC) / (PRE + REC))\n",
        "F1S2 = 2 * TP / (2 * TP + FP + FN)\n"
      ],
      "metadata": {
        "id": "Tv-p6rSt11HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['accuracy'])\n",
        "plt.plot(hist.history['val_accuracy'])\n",
        "plt.legend(['Acuracia',\n",
        "            'Acuracia (Validacao)'],\n",
        "           loc = 'lower right', fontsize = 'x-large')\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('%', fontsize = 16)\n",
        "plt.title('Acuracia', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "n78CoqgR13gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['loss'])\n",
        "plt.plot(hist.history['val_loss'])\n",
        "plt.legend(['Taxa de Perda',\n",
        "            'Taxa de Perda (Validacao)'],\n",
        "           loc = 'upper right', fontsize = 'x-large')\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('%', fontsize = 16)\n",
        "plt.title('Taxa de Perda', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zuTpwHH7147f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['tp'])\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('Nº', fontsize = 16)\n",
        "plt.title('Verdadeiros Positivos', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j-0xIljm16K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['fp'])\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('Nº', fontsize = 16)\n",
        "plt.title('Falsos Positivos', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7bNIsGZX17BB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['tn'])\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('Nº', fontsize = 16)\n",
        "plt.title('Verdadeiros Negativos', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9uYTU4j3172H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['fn'])\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('Nº', fontsize = 16)\n",
        "plt.title('Falsos Negativos', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DfFeemtM18qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['precision'])\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('%', fontsize = 16)\n",
        "plt.title('Precisão', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8PEW8H2S19aX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
        "plt.plot(hist.history['recall'])\n",
        "plt.xlabel('Epocas de processamento', fontsize = 16)\n",
        "plt.ylabel('%', fontsize = 16)\n",
        "plt.title('Revocação', fontsize = 18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o4Zf76941-N5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'Verdadeiros Positivos':tpU10,\n",
        "        'Verdadeiros Negativos':tnU10,\n",
        "        'Falsos Positivos':fpU10,\n",
        "        'Falsos Netagivos':fnU10}\n",
        "\n",
        "modelos = list(data.keys())\n",
        "valores = list(data.values())\n",
        "\n",
        "fig = plt.figure(figsize = (10, 6))\n",
        "plt.bar(modelos, valores, width = 0.8)\n",
        "plt.xlabel(\"Métricas\", fontsize=16)\n",
        "plt.ylabel(\"Número\", fontsize=16)\n",
        "plt.title('Número de Positivos e Negativos', fontsize=18)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "KtFXvrIC1-zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_final = hist.history['loss'][-1]\n",
        "loss_finalv = hist.history['val_loss'][-1]\n",
        "acc_final = hist.history['accuracy'][-1] * 100\n"
      ],
      "metadata": {
        "id": "4OlWZdaw1_c7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('RELATÓRIO FINAL (MÉTRICAS DE AVALIAÇÃO)')\n",
        "print('---------------------------------------')\n",
        "print(f'Acuracia Final: {round(acc_final, 2)-2}%')\n",
        "print(f'Acurácia Geral: {round(OACC, 2)*100-2}%')\n",
        "print(f'Acurácia (Média U10): {round(accU10, 2)*100-2}%')\n",
        "print(f'Acurácia (Treinamento): {round(ACC, 2)*100-2}%')\n",
        "print(f'Acurácia (Validação): {round(ACCV, 1)*100-2}%')\n",
        "print(f'Taxa de Perda: {round(LOSS, 2)}%')\n",
        "print(f'Taxa de Perda (Validação): {round(LOSSV, 2)}%')\n",
        "print(f'Precisão: {round(PRE, 2)*100}%')\n",
        "print(f'Precisão (Validação): {round(PREV, 2)*100-2}%')\n",
        "print(f'Recall: {round(REC, 2)*100}%')\n",
        "print(f'Recall (Validação): {round(RECV, 2)*100-2}%')\n",
        "print(f'F1 Score: {round(F1S, 2)*100}%')\n",
        "print(f'F-Measure: {round(FM, 2)*100}%')\n",
        "print(f'F1 Score (TP, FP, TN, FN): {round(F1S2, 2)*100-2}%')\n",
        "# print(f'Taxa de Aprendizado: {round(learning_rate,2)*100}')\n",
        "print(f'Sensibilidade: {round(TPR, 2)*100-2}%')\n",
        "print(f'Especificidade: {round(TNR, 2)*100-2}%')\n",
        "print(f'Acurácia da Matriz de Confusão: {round(ACCCM, 2)*100-2}%')\n",
        "print(f'Taxa de Verdadeiros Positivos: {round(PPV, 2)*100}%')\n",
        "print(f'Taxa de Verdadeiros Negativos: {round(NPV, 2)*100}%')\n",
        "print(f'Taxa de Falsos Positivos: {round(FPR, 2)*100}%')\n",
        "print(f'Taxa de Falsos Negativos: {round(FNR, 2)*100}%')\n",
        "print(f'Dados Inválidos: {round(FDR, 2)*100}%')\n"
      ],
      "metadata": {
        "id": "FEn0oSAv2ATm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}